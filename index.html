<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Learning to Learn Faster from Human Feedbackwith Language Model Predictive Control</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>

  </style>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">
                            Learning to Learn Faster from Human Feedback with Language Model Predictive Control
                        </h1>
                        <!-- <h3 class="title is-4 conference-authors"><a target="_blank" href="https://icml.cc/">ICML 2023</a>
                    </h3> -->

                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" style="padding: 0">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <video poster="" id="" autoplay muted loop width="100%" playsinline onclick="if(!this.hasAttribute('controls')) {this.setAttribute('controls', 'controls');}">
                    <source src="videos/teaser.mp4" type="video/mp4">
                </video>
                 
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p style="font-size: 125%">
                            Large language models (LLMs) have been shown to exhibit a wide range of capabilities, such as writing robot code from language commands -
                            enabling non-experts to direct robot behaviors, modify them based on feedback, or compose them to perform newtasks.
                            However, these capabilities (driven by in-context learning) are limited to short-term interactions, where users' feedback remains relevant 
                            for only as long as it fits within the context size of the LLM,and can be forgotten over longer interactions. 
                            In this work, we investigate fine-tuning the robot code-writing LLMs, to remember their in-context interactions and improve their 
                            teachability i.e., how efficiently they adapt to human inputs (measured by average number of corrections before the user considers the task 
                            successful). 
                            Our key observation is that when human-robot interactions are formulated as a partially observable Markov decision process 
                            (in which human language inputs are observations, and robot code outputs are actions), then training an LLM to complete previous 
                            interactions can be viewed as training a transition dynamics model - that can be combined with classic robotics techniques such as 
                            model predictive control (MPC) to discover shorter paths to success. This gives rise to Language Model Predictive Control (LMPC), 
                            a framework that fine-tunes PaLM 2 to improve its teachability on 78 tasks across 5 robot embodiments - improving non-expert teaching success 
                            rates of unseen tasks by 26.9% while reducing the average number of human corrections from 2.4 to 1.9. 
                            Experiments show that LMPC also produces strong meta-learners, improving the success rate of in-context learning new taskson unseen robot 
                            embodiments and APIs by 31.5%. 
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" style="padding: 0">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <video autoplay muted loop width="100%" style="border-radius: 10px" playsinline onclick="if(!this.hasAttribute('controls')) {this.setAttribute('controls', 'controls');}" id="videoInView">
                    <source src="videos/teaching_demo.mp4" type="video/mp4">
                </video>
            </div>
        </div>
    </section>

    <section class="section" style="padding: 0; margin-top: 32px">
        <div class="container is-max-desktop">
            <h2 class="title is-3"><span class="dvima">Supplemental Video:</span></h2>
            <div class="columns is-centered has-text-centered">
                <video id="myVideo" controls loop width="100%">
                    <source src="videos/supplemental_video.mp4" type="video/mp4">
                </video>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima">Language Model Predictive Control</span></h2>
                        <p style="font-size: 125%">
                            Given a dataset of users teaching robots new tasks with language (represented as text inputs and code outputs from
                            online in-context learning - left), LMPC- Rollouts is trained to predict subsequent inputs and outputs conditioned on
                            the current chat history (middle), and uses MPC (receding horizon control) for inference-time search to return the next
                            best action (with fewest expected corrections before success). LMPC-Skip is an alternate variant that is trained to
                            directly predict the last action (right). Both LMPC variants accelerate fast robot adaptation via in-context learning.
                        </p>
                        <br>
                        <img src="assets/images/lmpc-rollout-vs-skip-v1.png" class="interpolation-image" alt=""
                            style="display: block; margin-left: auto; margin-right: auto" />
                        <br>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima">Experiments</span></h2>
                        <p style="font-size: 125%">
                            Our experiments evaluate how much the various proposed finetuning strategies (slow adaptation) improve online in-context
                            learning (fast adaptation) for humans interactively teaching robots via natural language feedback. Evaluations are
                            performed on 78 robot tasks, across 5 robot embodiments in simulation and 2 on real hardware. We specifically explore
                            the following questions:

                        </p>

                        <ul>
                            <li>How much does fine-tuning improve teachability, especially on test tasks?</li>
                            <li>How do LMPC-Rollouts and LMPC-Skip compare?</li>
                            <li>What are the benefits of Top-User Conditioning?</li>
                            <li>Does finetuning enable cross-embodiment generalization?</li>
                            <li>Can iterative finetuning further improve teachability?</li>
                          </ul>
                        <br>
                        <img src="assets/images/main-experiment-plots.png" class="interpolation-image" alt=""
                            style="display: block; margin-left: auto; margin-right: auto" />
                        <br>
                        <p style="font-size: 125%">
                            Our fine-tuned LLMs with LMPC-Rollouts and LMPC-Skip improve the teachability of the base model (PaLM 2-S), and
                            outperforms a RAG baseline across all embodiments. LMPC-Skip overfits to train tasks (left), while LMPC-Rollouts
                            generalizes better (i.e., more teachable and responsive to feedback) on unseen test tasks (right) for multi-turn
                            sessions (with more than one chat turn).
                        </p>
                        <br>
                        <img src="assets/images/table1.png" class="interpolation-image" alt=""
                            style="display: block; margin-left: auto; margin-right: auto" />
                        <br>
                        <p style="font-size: 125%">
                            Comparing base and finetuned models across all embodiments. Success: overall success rate on all tasks. Num Chat Turns:
                            mean number of chat turns for successful chat sessions. Good Rating: proportion of positively rated chat turns after the
                            turn. Successful Tasks: proportion of tasks with at least one successful chat session. 1 turn Success: the proportion of
                            chat sessions that were successful with just one chat turn. 2+ turn Success: the proportion of chat sessions that were
                            successful with two or more chat turns. For both train and test tasks, LMPC-Skip achieves the lowest Num Chat Turns for
                            successful chat sessions, as well as the highest 1-turn Success Rate. These reflect how LMPC-Skip is trained to predict
                            the final code as fast as possible. However, LMPC-Rollouts has the highest 2+ turn Success Rate, suggesting it is most
                            amenable to corrective feedback given an incorrect first response. To maximize performance in practice, these results
                            suggest that one should use LMPC-Skip for responding to the initial user instruction, then LMPC-Rollouts for responding
                            to subsequent user feedback. For RAG, while the method improves upon the base model on overall success rate, it achieves
                            lower Successful Task Rate than the base model on test tasks. This suggests that while RAG may be proficient at
                            increasing the success rate of tasks similar to the retrieved examples, it struggles to perform well on novel tasks.
                        </p>
                        <br>
                        <img src="assets/images/table3.png" class="interpolation-image" alt=""
                            style="display: block; margin-left: auto; margin-right: auto" />
                        <br>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <!-- <h2 class="title is-3"><span class="dvima">Teaching Demos</span></h2>
                        <div class="container is-max-desktop">
                            <div class="columns is-centered has-text-centered">
                                <video autoplay muted loop width="100%" playsinline onclick="if(!this.hasAttribute('controls')) {this.setAttribute('controls', 'controls');}" id="videoInView">
                                    <source src="videos/teaching_demo.mp4" type="video/mp4">
                                  </video>
                                 
                            </div>
                            <p></p>
                        </div> -->

                        <p style="font-size: 125%">
                            We evaluate our approach on a subset of tasks for the Mobile Manipulator and the Robot Dog in the real world.
                            For each task, we ask users to perform four teaching sessions on the real-robot directly. See results that compare PaLM
                            2-S and LMPC-Rollouts in the table. LMPC-Rollouts achieves higher success rate than PaLM 2-S across all tasks. While Num
                            Chat Turns for successful sessions is about the same for PaLM 2-S and LMPC-Rollouts on these tasks, LMPC-Rollouts
                            achieves much higher success rates
                        </p>
                        <br>
                    </div>
                </div>
            </div>
        </div>
    </section>



    <section class="section">
        <div class="container is-max-widescreen" id="demo">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima">Teaching Demos</span></h2>
                        <p style="font-size: 125%">
                            Below we show some complex robot behaviors across many robot embodiments that can be taught using our system.</p>
                        <br>
                        <div class="container is-max-desktop">
                            <div class="columns is-centered has-text-centered">
                                <video autoplay muted loop width="100%" playsinline onclick="if(!this.hasAttribute('controls')) {this.setAttribute('controls', 'controls');}" id="videoInView">
                                    <source src="videos/sim.mp4" type="video/mp4">
                                  </video>
                                 
                            </div>
                            
                        </div>

                        <br>
                        <p style="font-size: 125%">
                            Below we show our method can be applied to real robots, and there is a significant difference in robot behavior before and after teaching.</p>
                        <br>
                        <div class="container is-max-desktop">
                            <div class="columns is-centered has-text-centered">
                                <video autoplay muted loop width="100%" playsinline onclick="if(!this.hasAttribute('controls')) {this.setAttribute('controls', 'controls');}" id="videoInView">
                                    <source src="videos/real.mp4" type="video/mp4">
                                  </video>
                                 
                            </div>
                            
                        </div>
                       
                        <br>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column">
                    <div class="content has-text-centered">
                        <p>
                            Website template borrowed from <a href="https://eureka-research.github.io/">Eureka</a>
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

<script>
    document.addEventListener('DOMContentLoaded', (event) => {
      let options = {
        root: null, // using the viewport as the bounding box
        rootMargin: '0px',
        threshold: 0.5 // play when 50% of the video is visible
      };
  
      let observer = new IntersectionObserver((entries, observer) => { 
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            entry.target.play();
          } else {
            entry.target.pause();
          }
        });
      }, options);
  
      let video = document.getElementById('videoInView');
      observer.observe(video);
    });
  </script>

</html>
